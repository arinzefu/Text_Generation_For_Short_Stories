{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('short_stories.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:54.916095200Z",
     "start_time": "2023-12-01T10:09:54.898884Z"
    }
   },
   "id": "7f664e96d1cb7903"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "stories = text.split('-END OF TEXT-')\n",
    "stories = [story.strip() for story in stories if story.strip()] "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:54.932107500Z",
     "start_time": "2023-12-01T10:09:54.916095200Z"
    }
   },
   "id": "f942eb01f8ca5225"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stories: 138\n"
     ]
    }
   ],
   "source": [
    "num_stories = len(stories)\n",
    "\n",
    "print(f'Total number of stories: {num_stories}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:54.948888300Z",
     "start_time": "2023-12-01T10:09:54.932107500Z"
    }
   },
   "id": "58a2b3d5241c463a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:55.048985Z",
     "start_time": "2023-12-01T10:09:54.948888300Z"
    }
   },
   "id": "15327afd9185176d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "3655"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words #number of unique words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:55.065274200Z",
     "start_time": "2023-12-01T10:09:55.048985Z"
    }
   },
   "id": "e083ebf7c0521006"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "sequence_length = 15\n",
    "# using the last 15 words to predict the next word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:55.114663800Z",
     "start_time": "2023-12-01T10:09:55.065274200Z"
    }
   },
   "id": "9282df7cddcb6b11"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('-END OF TEXT-'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(sequence_length, len(token_list)):\n",
    "        n_gram_sequence = token_list[i-sequence_length:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:55.115317700Z",
     "start_time": "2023-12-01T10:09:55.082248500Z"
    }
   },
   "id": "ca307029835633c9"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "max_sequence_length = sequence_length + 1  \n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:55.165589200Z",
     "start_time": "2023-12-01T10:09:55.115317700Z"
    }
   },
   "id": "de32b9226833a807"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:55.198961700Z",
     "start_time": "2023-12-01T10:09:55.165589200Z"
    }
   },
   "id": "7089a07182814618"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(13808, 15)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:55.215814100Z",
     "start_time": "2023-12-01T10:09:55.198961700Z"
    }
   },
   "id": "6bec89d2a45e9b76"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(13808, 3655)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T10:09:55.264473200Z",
     "start_time": "2023-12-01T10:09:55.215814100Z"
    }
   },
   "id": "566331c052ffa331"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Dropout\n",
    "\n",
    "embedding_dim = 100\n",
    "lstm_size = 128\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Embedding(input_dim=total_words, output_dim=embedding_dim, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(lstm_size, return_sequences=True))\n",
    "model.add(LSTM(lstm_size))\n",
    "model.add(Dense(lstm_size, activation = \"relu\"))\n",
    "\n",
    "model.add(Dense(total_words, activation='softmax'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:33:31.709376500Z",
     "start_time": "2023-12-01T11:33:31.236970800Z"
    }
   },
   "id": "f01366d0f28de406"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:33:31.747390Z",
     "start_time": "2023-12-01T11:33:31.712368400Z"
    }
   },
   "id": "ae6eb65a8e149af2"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 15, 100)           365500    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 15, 128)           117248    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3655)              471495    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1102339 (4.21 MB)\n",
      "Trainable params: 1102339 (4.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:33:31.799796800Z",
     "start_time": "2023-12-01T11:33:31.742404Z"
    }
   },
   "id": "51bf0d4eb16ad1e1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:33:31.799796800Z",
     "start_time": "2023-12-01T11:33:31.772836300Z"
    }
   },
   "id": "c157489dcdfa6810"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "432/432 [==============================] - 13s 21ms/step - loss: 6.9043 - accuracy: 0.0545\n",
      "Epoch 2/300\n",
      "432/432 [==============================] - 9s 21ms/step - loss: 6.4734 - accuracy: 0.0548\n",
      "Epoch 3/300\n",
      "432/432 [==============================] - 10s 22ms/step - loss: 6.2971 - accuracy: 0.0579\n",
      "Epoch 4/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 6.1252 - accuracy: 0.0641\n",
      "Epoch 5/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 5.9724 - accuracy: 0.0671\n",
      "Epoch 6/300\n",
      "432/432 [==============================] - 11s 27ms/step - loss: 5.8362 - accuracy: 0.0720\n",
      "Epoch 7/300\n",
      "432/432 [==============================] - 14s 32ms/step - loss: 5.6837 - accuracy: 0.0833\n",
      "Epoch 8/300\n",
      "432/432 [==============================] - 13s 31ms/step - loss: 5.5288 - accuracy: 0.0932\n",
      "Epoch 9/300\n",
      "432/432 [==============================] - 14s 33ms/step - loss: 5.3803 - accuracy: 0.1028\n",
      "Epoch 10/300\n",
      "432/432 [==============================] - 13s 29ms/step - loss: 5.2398 - accuracy: 0.1112\n",
      "Epoch 11/300\n",
      "432/432 [==============================] - 13s 30ms/step - loss: 5.1015 - accuracy: 0.1147\n",
      "Epoch 12/300\n",
      "432/432 [==============================] - 13s 31ms/step - loss: 4.9581 - accuracy: 0.1236\n",
      "Epoch 13/300\n",
      "432/432 [==============================] - 17s 39ms/step - loss: 4.8142 - accuracy: 0.1296\n",
      "Epoch 14/300\n",
      "432/432 [==============================] - 15s 34ms/step - loss: 4.6716 - accuracy: 0.1372\n",
      "Epoch 15/300\n",
      "432/432 [==============================] - 15s 34ms/step - loss: 4.5297 - accuracy: 0.1438\n",
      "Epoch 16/300\n",
      "432/432 [==============================] - 13s 31ms/step - loss: 4.3889 - accuracy: 0.1535\n",
      "Epoch 17/300\n",
      "432/432 [==============================] - 14s 32ms/step - loss: 4.2528 - accuracy: 0.1629\n",
      "Epoch 18/300\n",
      "432/432 [==============================] - 14s 33ms/step - loss: 4.1255 - accuracy: 0.1696\n",
      "Epoch 19/300\n",
      "432/432 [==============================] - 13s 31ms/step - loss: 3.9933 - accuracy: 0.1774\n",
      "Epoch 20/300\n",
      "432/432 [==============================] - 13s 31ms/step - loss: 3.8562 - accuracy: 0.1870\n",
      "Epoch 21/300\n",
      "432/432 [==============================] - 13s 30ms/step - loss: 3.7302 - accuracy: 0.2029\n",
      "Epoch 22/300\n",
      "432/432 [==============================] - 13s 29ms/step - loss: 3.6051 - accuracy: 0.2142\n",
      "Epoch 23/300\n",
      "432/432 [==============================] - 13s 29ms/step - loss: 3.4815 - accuracy: 0.2285\n",
      "Epoch 24/300\n",
      "432/432 [==============================] - 13s 30ms/step - loss: 3.3556 - accuracy: 0.2445\n",
      "Epoch 25/300\n",
      "432/432 [==============================] - 12s 29ms/step - loss: 3.2438 - accuracy: 0.2637\n",
      "Epoch 26/300\n",
      "432/432 [==============================] - 15s 36ms/step - loss: 3.1318 - accuracy: 0.2740\n",
      "Epoch 27/300\n",
      "432/432 [==============================] - 13s 29ms/step - loss: 3.0213 - accuracy: 0.2951\n",
      "Epoch 28/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 2.9328 - accuracy: 0.3107\n",
      "Epoch 29/300\n",
      "432/432 [==============================] - 12s 29ms/step - loss: 2.8114 - accuracy: 0.3325\n",
      "Epoch 30/300\n",
      "432/432 [==============================] - 12s 29ms/step - loss: 2.7078 - accuracy: 0.3491\n",
      "Epoch 31/300\n",
      "432/432 [==============================] - 12s 29ms/step - loss: 2.6149 - accuracy: 0.3617\n",
      "Epoch 32/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 2.5349 - accuracy: 0.3840\n",
      "Epoch 33/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 2.4446 - accuracy: 0.4003\n",
      "Epoch 34/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 2.3618 - accuracy: 0.4108\n",
      "Epoch 35/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 2.2889 - accuracy: 0.4253\n",
      "Epoch 36/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 2.2019 - accuracy: 0.4480\n",
      "Epoch 37/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 2.1275 - accuracy: 0.4646\n",
      "Epoch 38/300\n",
      "432/432 [==============================] - 12s 29ms/step - loss: 2.0631 - accuracy: 0.4784\n",
      "Epoch 39/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 1.9860 - accuracy: 0.4963\n",
      "Epoch 40/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 1.9185 - accuracy: 0.5100\n",
      "Epoch 41/300\n",
      "432/432 [==============================] - 12s 29ms/step - loss: 1.8571 - accuracy: 0.5229\n",
      "Epoch 42/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 1.7936 - accuracy: 0.5393\n",
      "Epoch 43/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 1.7425 - accuracy: 0.5461\n",
      "Epoch 44/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.6707 - accuracy: 0.5655\n",
      "Epoch 45/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.6144 - accuracy: 0.5770\n",
      "Epoch 46/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 1.5500 - accuracy: 0.5951\n",
      "Epoch 47/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.4921 - accuracy: 0.6084\n",
      "Epoch 48/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.4461 - accuracy: 0.6187\n",
      "Epoch 49/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.3986 - accuracy: 0.6349\n",
      "Epoch 50/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.3355 - accuracy: 0.6503\n",
      "Epoch 51/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 1.2828 - accuracy: 0.6582\n",
      "Epoch 52/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.2539 - accuracy: 0.6649\n",
      "Epoch 53/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 1.2024 - accuracy: 0.6773\n",
      "Epoch 54/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.1592 - accuracy: 0.6913\n",
      "Epoch 55/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.1083 - accuracy: 0.7039\n",
      "Epoch 56/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 1.0691 - accuracy: 0.7121\n",
      "Epoch 57/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.0234 - accuracy: 0.7241\n",
      "Epoch 58/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.9725 - accuracy: 0.7413\n",
      "Epoch 59/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.9279 - accuracy: 0.7560\n",
      "Epoch 60/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.9095 - accuracy: 0.7562\n",
      "Epoch 61/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.8736 - accuracy: 0.7690\n",
      "Epoch 62/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.8208 - accuracy: 0.7853\n",
      "Epoch 63/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.7862 - accuracy: 0.7927\n",
      "Epoch 64/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.7593 - accuracy: 0.7953\n",
      "Epoch 65/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.7295 - accuracy: 0.8072\n",
      "Epoch 66/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.7150 - accuracy: 0.8105\n",
      "Epoch 67/300\n",
      "432/432 [==============================] - 12s 27ms/step - loss: 0.6657 - accuracy: 0.8222\n",
      "Epoch 68/300\n",
      "432/432 [==============================] - 11s 27ms/step - loss: 0.6190 - accuracy: 0.8370\n",
      "Epoch 69/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.5878 - accuracy: 0.8469\n",
      "Epoch 70/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.5960 - accuracy: 0.8397\n",
      "Epoch 71/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.5871 - accuracy: 0.8420\n",
      "Epoch 72/300\n",
      "432/432 [==============================] - 12s 28ms/step - loss: 0.5284 - accuracy: 0.8612\n",
      "Epoch 73/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.4873 - accuracy: 0.8732\n",
      "Epoch 74/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.4663 - accuracy: 0.8818\n",
      "Epoch 75/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.4566 - accuracy: 0.8838\n",
      "Epoch 76/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.4401 - accuracy: 0.8843\n",
      "Epoch 77/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.4328 - accuracy: 0.8893\n",
      "Epoch 78/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.4047 - accuracy: 0.8980\n",
      "Epoch 79/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.3860 - accuracy: 0.9024\n",
      "Epoch 80/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.3337 - accuracy: 0.9210\n",
      "Epoch 81/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.3186 - accuracy: 0.9235\n",
      "Epoch 82/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.3666 - accuracy: 0.9046\n",
      "Epoch 83/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.3552 - accuracy: 0.9056\n",
      "Epoch 84/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.3101 - accuracy: 0.9241\n",
      "Epoch 85/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.2747 - accuracy: 0.9368\n",
      "Epoch 86/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.2629 - accuracy: 0.9366\n",
      "Epoch 87/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.2540 - accuracy: 0.9416\n",
      "Epoch 88/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.2668 - accuracy: 0.9358\n",
      "Epoch 89/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.2501 - accuracy: 0.9395\n",
      "Epoch 90/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.2487 - accuracy: 0.9368\n",
      "Epoch 91/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.2130 - accuracy: 0.9482\n",
      "Epoch 92/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.2054 - accuracy: 0.9523\n",
      "Epoch 93/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1827 - accuracy: 0.9618\n",
      "Epoch 94/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1624 - accuracy: 0.9649\n",
      "Epoch 95/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1970 - accuracy: 0.9518\n",
      "Epoch 96/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.2259 - accuracy: 0.9440\n",
      "Epoch 97/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.2110 - accuracy: 0.9470\n",
      "Epoch 98/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1946 - accuracy: 0.9506\n",
      "Epoch 99/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.1734 - accuracy: 0.9596\n",
      "Epoch 100/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1416 - accuracy: 0.9702\n",
      "Epoch 101/300\n",
      "432/432 [==============================] - 11s 27ms/step - loss: 0.1082 - accuracy: 0.9804\n",
      "Epoch 102/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1447 - accuracy: 0.9665\n",
      "Epoch 103/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1854 - accuracy: 0.9508\n",
      "Epoch 104/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.1766 - accuracy: 0.9552\n",
      "Epoch 105/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1706 - accuracy: 0.9581\n",
      "Epoch 106/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1266 - accuracy: 0.9712\n",
      "Epoch 107/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0849 - accuracy: 0.9852\n",
      "Epoch 108/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0579 - accuracy: 0.9926\n",
      "Epoch 109/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0660 - accuracy: 0.9898\n",
      "Epoch 110/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1912 - accuracy: 0.9503\n",
      "Epoch 111/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.3021 - accuracy: 0.9097\n",
      "Epoch 112/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1946 - accuracy: 0.9470\n",
      "Epoch 113/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.1013 - accuracy: 0.9781\n",
      "Epoch 114/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0513 - accuracy: 0.9933\n",
      "Epoch 115/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0298 - accuracy: 0.9979\n",
      "Epoch 116/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0201 - accuracy: 0.9988\n",
      "Epoch 117/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0133 - accuracy: 0.9998\n",
      "Epoch 118/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 119/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 120/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 121/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0079 - accuracy: 0.9999\n",
      "Epoch 122/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.7391 - accuracy: 0.8180\n",
      "Epoch 123/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.4918 - accuracy: 0.8573\n",
      "Epoch 124/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.1244 - accuracy: 0.9697\n",
      "Epoch 125/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0357 - accuracy: 0.9964\n",
      "Epoch 126/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0167 - accuracy: 0.9996\n",
      "Epoch 127/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 128/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 129/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 130/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 131/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 132/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 133/300\n",
      "432/432 [==============================] - 11s 27ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 134/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 135/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0240 - accuracy: 0.9947\n",
      "Epoch 136/300\n",
      "432/432 [==============================] - 11s 25ms/step - loss: 1.0463 - accuracy: 0.7383\n",
      "Epoch 137/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.2932 - accuracy: 0.9155\n",
      "Epoch 138/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0752 - accuracy: 0.9852\n",
      "Epoch 139/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0245 - accuracy: 0.9978\n",
      "Epoch 140/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 141/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 142/300\n",
      "432/432 [==============================] - 11s 26ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 143/300\n",
      "432/432 [==============================] - 11s 27ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 144/300\n",
      "432/432 [==============================] - 12s 29ms/step - loss: 0.0049 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x1f564936440>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model.fit(X, y, epochs=300, verbose=1, callbacks=[early_stopping])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:01:19.413416500Z",
     "start_time": "2023-12-01T11:33:31.787798600Z"
    }
   },
   "id": "360794e027c8c6f8"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Today is a beautiful day to  a hundred message it i was no one nevermore to return where you once seen someone my temporary hazel was decided to get the shot of sleep blocking hormones in order to stay awake all winter and guard the sleeping people i don't even mind the neurological damage caused by the sleep blocking hormone shot i don't even mind that i'm likely to die soon like almost all guards i'm just afraid for my species to follow me to the grave one of these winters next down and darkness you're hate said on mars writhing in pain on cots and\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Today is a beautiful day to \"\n",
    "next_words = 100\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:01:26.772370100Z",
     "start_time": "2023-12-01T12:01:19.429031700Z"
    }
   },
   "id": "c7d71dba5187081b"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_text(seed_text):\n",
    "    next_words = 100\n",
    "    generated_text = seed_text\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "\n",
    "        seed_text += \" \" + output_word\n",
    "        generated_text += \" \" + output_word\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "iface = gr.Interface(fn=generate_text, inputs=\"text\", outputs=\"text\")\n",
    "iface.launch()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T12:51:27.381485200Z",
     "start_time": "2023-12-01T12:51:09.924629700Z"
    }
   },
   "id": "6bdc483d3ab08f58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
